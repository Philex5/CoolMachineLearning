# 1.2逻辑回归

## 逻辑回归的定义

逻辑回归（Logistic Regression）也称为"对数几率回归"，又称为"逻辑斯谛"回归。是一个对数线性分类模型，虽然有*回归*两个字，但属于分类模型而不是回归模型。

### 逻辑回归模型为:

$$P(Y=1|x) = \frac{e^{w\cdot x }}{1 + e^{w\cdot x}}$$

$$P(Y=0|x) = \frac{1}{1 + e^{w\cdot x }}$$

$$其中 b = w_0,x_0 = 1$$



## 逻辑回归的本质

**几率(odd):** 指事件发生的概率与该事件不发生的概率之间的比值。

事件的对数几率或logit函数为：

$$logit(p) = log \frac{p}{1-p}$$

对于逻辑回归:

 $$logit(P(Y=1|x)) = log \frac{P(Y=1|x)}{1-P(Y=1|x)} = w\cdot x$$

所以逻辑回归模型的本质是：**输出Y=1的对数几率是输入x的线性函数表示的模型。** 逻辑回归是使用**线性预测的结果去逼近真实标记的对数几率**。这也是为什么被称为“对数几率回归”的原因。



## 逻辑回归的参数估计

逻辑回归相当于线性预测结果经过sigmoid激活函数，得到类别概率分布。

$$sigmoid = \frac {1}{1+e^{-x}}$$



这里使用最大似然法进行参数估计，学习使得当前标记序列出现概率最大的权重向量$W$.



### 似然函数

设$$\phi()=sigmoid, z^{(i)}= w \cdot x^{(i)}$$

$$L(w) = P(y| x; w) = \prod_{i=1}^{n}((\phi (z^{(i)}))^{y^{(i)}}((1-\phi (z^{(i)}))^{1-y^{(i)}}),y \subseteq ({1,0})$$

则逻辑回归的**损失函数**为：

$$Loss(\phi(z), y) = - logL(w) = \sum_{i=1}^n(- y^{(i)}log(\phi(z^{(i)}))-(1-y^{(i)})log(1-\phi(z^{(i)})))$$

将最大化似然函数改为最小化对数似然即损失函数$Loss$.(*其实就是Cross_Entropy*)



### 参数学习

逻辑回归模型的参数学习通常采用的方法是梯度下降法和拟牛顿法

#### 梯度下降法

参数更新：

$$w_j := w_j + \eta \sum_{i=1}^n (y^{(i)}-\phi(z^{(i)}))x_j$$

**代码实现(Python):**



每次更新参数使用全部样本的梯度下降更新幅度更大，但可能会收敛不到全局最优点，在最优点两边跳动，而且如果样本数量太大消耗太大；每次更新参数使用一个样本的随机梯度下降更新的频率更快，收敛得也更快，也可以跳过局部最优点，可以用于在线学习，应用得更广泛；还有一种改进的随机梯度下降是每次随机选取更新参数用的样本。

#### 牛顿法：





## 逻辑回归的优缺点

## 逻辑回归 vs 线性回归



